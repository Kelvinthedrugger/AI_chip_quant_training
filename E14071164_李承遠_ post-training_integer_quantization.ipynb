{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8E0lw5eYWm"
      },
      "source": [
        "# Post-training integer quantization with 16 bit activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTaMbxioAsV0"
      },
      "source": [
        "#### 16 bit on activation has been suggested to cut down the model size while mainitaining high accuracy as much as possible.\n",
        "\n",
        "#### For comparison, we would show the storage size and accuracy of a model\n",
        "#### with no optimization (no additional attributes added to converter() class)\n",
        "#### and the optimized with 16-bit activation one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqqUIZjZjac"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0nR5AMEWq0H"
      },
      "source": [
        "In order to quantize both the input and output tensors, we need to use APIs added in TensorFlow r2.3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WsN6s5L1ieNl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "assert float(tf.__version__[:3]) >= 2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eMsw_6HujaqM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# get Fasion Mnist dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "test_images = test_images.astype(np.float32) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ny_xqlOL8asq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(15,activation='relu'),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf9OMBtz8e_k",
        "outputId": "9de1dfeb-abfe-49ee-b4dd-cf004c5d24b1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 22s 10ms/step - loss: 0.4611 - accuracy: 0.8368 - val_loss: 0.3839 - val_accuracy: 0.8625\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3069 - accuracy: 0.8871 - val_loss: 0.3466 - val_accuracy: 0.8731\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2551 - accuracy: 0.9064 - val_loss: 0.3358 - val_accuracy: 0.8868\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2204 - accuracy: 0.9189 - val_loss: 0.3538 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1952 - accuracy: 0.9269 - val_loss: 0.3507 - val_accuracy: 0.8846\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11f08bbe50>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                  from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=5,\n",
        "  validation_data=(test_images, test_labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TDe8MPpZkNhl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "_, pretrained = tempfile.mkstemp('.tf')\n",
        "\n",
        "model.save_weights(pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bXgecy06j56B",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(15,activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTczqX8hj5Ye",
        "outputId": "ed170db6-53b4-4f13-a11b-3e64d6f01c60",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 0.3507 - accuracy: 0.8846\n"
          ]
        }
      ],
      "source": [
        "def get_model():\n",
        "  model = build_model()\n",
        "  model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "  model.load_weights(pretrained)\n",
        "  return model\n",
        "\n",
        "# test if it works\n",
        "_m = get_model()\n",
        "_ = _m.evaluate(test_images,test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuTEoGFYd8aM"
      },
      "source": [
        "## Convert to a TensorFlow Lite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewi2f5Ll-V1m"
      },
      "source": [
        "### Convert model to a tflite model with no optimization\n",
        "\n",
        "#### As a comparision to our optimized model in the following cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7s2YJ15go6J",
        "outputId": "167114dc-757d-406d-dbeb-a6dbc283af48",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp5zw9ozmd/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQgTqbvPvxGJ"
      },
      "source": [
        "### Convert using weight-8-activation-16 quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwR9keYAwArA"
      },
      "source": [
        "To quantize the input and output tensors, and make the converter throw an error if it encounters an operation it cannot quantize, convert the model again with some additional parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzjEjcDs3BHa",
        "outputId": "c4b3c974-89fe-4f79-a7c5-8e223e9bc0d0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpb4lpnlnm/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpb4lpnlnm/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# the 2nd term ensures that if there are unsupported\n",
        "# ops that can be convert to intOps, leave them as floatOps\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYd6NxD03yjB"
      },
      "source": [
        "The internal quantization remains the same as above, but you can see the input and output tensors are now integer format:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaNkOS-twz4k",
        "outputId": "f13553d6-fdc3-4976-a16e-db152ae0c37c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:  <class 'numpy.float32'>\n",
            "output:  <class 'numpy.float32'>\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sse224YJ4KMm"
      },
      "source": [
        "### Save the models as files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_9nZ4nv4b9P"
      },
      "source": [
        "You'll need a `.tflite` file to deploy your model on other devices. So let's save the converted models to files and then load them when we run inferences below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEY59dC14uRv",
        "outputId": "61836485-e300-49cb-ed84-d203bd5b2350",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float model size: 282940\n",
            "quantized model size: 76688\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/fmnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "tflite_model_file = tflite_models_dir/\"fmnist_model.tflite\"\n",
        "unquantized_size = tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_file = tflite_models_dir/\"fmnist_model_quant.tflite\"\n",
        "quantized_size = tflite_model_quant_file.write_bytes(tflite_model_quant)\n",
        "\n",
        "print(\"float model size: %5d\\nquantized model size: %5d\" % (unquantized_size,quantized_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoixHuOQDXeb"
      },
      "source": [
        "Check the model size using shell command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS32tIB-DT1E",
        "outputId": "ca5373dc-0dde-4297-f612-38b61eee5720",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 576K\n",
            "-rw-r--r-- 1 root root 143K Apr  1 14:44 fmnist_model_quant_float.tflite\n",
            "-rw-r--r-- 1 root root  74K Apr  1 14:44 fmnist_model_quant_intonly.tflite\n",
            "-rw-r--r-- 1 root root  75K Apr  1 14:47 fmnist_model_quant.tflite\n",
            "-rw-r--r-- 1 root root 277K Apr  1 14:47 fmnist_model.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {tflite_models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t9yaTeF9fyM"
      },
      "source": [
        "## Run the TensorFlow Lite models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8lQHMp_asCq"
      },
      "source": [
        "Now we'll run inferences via TensorFlow Lite Interpreter to compare the model accuracies.\n",
        "\n",
        "Define a function that runs inference with a given model and images, and then returns the predictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X092SbeWfd1A",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_images\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = test_images[test_image_index]\n",
        "    test_label = test_labels[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "\n",
        "  return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2opUt_JTdyEu"
      },
      "source": [
        "### Test the models on one image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpPpFPaz7eEM"
      },
      "source": [
        "Remember, `tflite_model_file` is the original TensorFlow Lite model with floating-point data,\n",
        "\n",
        "while `tflite_model_quant_file` is the last model we converted using integer-only quantization (it uses uint8 data for input and output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zR2cHRUcUZ6e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "def test_model(tflite_file, test_image_index, model_type):\n",
        "  global test_labels\n",
        "\n",
        "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
        "\n",
        "  plt.imshow(test_images[test_image_index])\n",
        "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
        "  _ = plt.title(template.format(true= str(test_labels[test_image_index]), predict=str(predictions[0])))\n",
        "  plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5OTJ_6Vcslt"
      },
      "source": [
        "Now test the float model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1pTO92cEEUb9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# set the image to check\n",
        "test_image_index = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "iTK0x980coto",
        "outputId": "83b3c859-4947-49ed-ff6a-70889e7c9f0e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_model(tflite_model_file, test_image_index, model_type=\"Float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3N6-UGl1dfE"
      },
      "source": [
        "And test the quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "rc1i9umMcp0t",
        "outputId": "dedd0e0e-5d16-47f1-9e3e-16e087092b05",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_model(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwN7uIdCd8Gw"
      },
      "source": [
        "### Evaluate the models on all images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFKOD4DG8XmU"
      },
      "source": [
        "Now let's run both models using all the test images we loaded at the beginning of this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "05aeAuWjvjPx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_model(tflite_file, model_type):\n",
        "  global test_images\n",
        "  global test_labels\n",
        "\n",
        "  test_image_indices = range(test_images.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "\n",
        "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
        "\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
        "      model_type, accuracy, len(test_images)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnFilQpBuMh5"
      },
      "source": [
        "Evaluate the float model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5mWkSbMcU5z",
        "outputId": "feaf13a0-5e61-4f3b-d8a1-e21ebc7397dd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model accuracy is 88.4600% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_model_file, model_type=\"Float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km3cY9ry8ZlG"
      },
      "source": [
        "Evaluate the quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9cnwiPp6EGm",
        "outputId": "b799ef1f-8ef2-4ce6-8dde-1e7c5b9af2d0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized model accuracy is 88.4200% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BOQBQOGD-IB"
      },
      "source": [
        "### As the result shown, the quantized model shown little difference in accuracy\n",
        "### and 75% model size reduction compare to the original model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVg3nX4Eflvs"
      },
      "source": [
        "### Integer only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC2HNTOEAUrJ",
        "outputId": "f28b5db7-13c5-43e8-b0dd-415c01defa31",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp7qygmhay/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp7qygmhay/assets\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:  <class 'numpy.uint8'>\n",
            "output:  <class 'numpy.uint8'>\n",
            "quantized int only model size: 75768\n",
            "Quantized_int_only model accuracy is 88.5100% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant_intonly = converter.convert()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_intonly)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)\n",
        "\n",
        "# storage size\n",
        "tflite_model_quant_intonly_file = tflite_models_dir/\"fmnist_model_quant_intonly.tflite\"\n",
        "quantized_intonly_size = tflite_model_quant_intonly_file.write_bytes(tflite_model_quant_intonly)\n",
        "\n",
        "print(\"quantized int only model size: %5d\" % (quantized_intonly_size))\n",
        "\n",
        "evaluate_model(tflite_model_quant_intonly_file, model_type='Quantized_int_only')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fML8QK3sfpZ2"
      },
      "source": [
        "### Float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6_8sbfwfrBU",
        "outputId": "c9355fcf-be4b-4a2a-b6ee-8d8f9f044858",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpor8e0_ad/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpor8e0_ad/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:  <class 'numpy.float32'>\n",
            "output:  <class 'numpy.float32'>\n",
            "quantized int only model size: 145664\n",
            "Quantized_float_only model accuracy is 88.4600% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "tflite_model_quant_float = converter.convert()\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)\n",
        "\n",
        "# storage size\n",
        "tflite_model_quant_float_file = tflite_models_dir/\"fmnist_model_quant_float.tflite\"\n",
        "quantized_float_size = tflite_model_quant_float_file.write_bytes(tflite_model_quant_float)\n",
        "\n",
        "print(\"quantized int only model size: %5d\" % (quantized_float_size))\n",
        "\n",
        "evaluate_model(tflite_model_quant_float_file, model_type='Quantized_float_only')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4C2SJ80ijitq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "post_training_integer_quant_final_exp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
